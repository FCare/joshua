# Joshua - AI Assistant Frontend

Une interface moderne et Ã©purÃ©e pour interagir avec des modÃ¨les AI, extraite et adaptÃ©e du frontend llama.cpp.

![Joshua Interface](https://img.shields.io/badge/Frontend-Modern%20Chat-blue)
![License](https://img.shields.io/badge/License-MIT-green)

## âœ¨ FonctionnalitÃ©s

- **Interface Ã©purÃ©e** - Design moderne inspirÃ© de ChatGPT
- **Chat en temps rÃ©el** - Streaming des rÃ©ponses
- **Upload de fichiers** - Support images et documents
- **Mode adaptatif** - Responsive design
- **API compatible** - Fonctionne avec llama.cpp et autres backends
- **Mode dÃ©mo** - Fonctionne sans backend pour les tests

## ğŸš€ DÃ©marrage rapide

### Option 1: Serveur Python intÃ©grÃ©

```bash
cd joshua-frontend
python3 server.py [port] [backend-url]
```

**Exemples:**
```bash
# Serveur sur port 8080, backend llama.cpp sur localhost:8080
python3 server.py

# Serveur sur port 3000, backend custom
python3 server.py 3000 http://localhost:11434

# Mode dÃ©mo (sans backend)
python3 server.py 8080 http://localhost:9999
```

### Option 2: Serveur web statique

Servez les fichiers via n'importe quel serveur web :

```bash
# Avec Python
python3 -m http.server 8080

# Avec Node.js
npx http-server -p 8080

# Avec PHP
php -S localhost:8080
```

Puis ouvrez: `http://localhost:8080`

## ğŸ“ Structure du projet

```
joshua-frontend/
â”œâ”€â”€ index.html          # Interface principale
â”œâ”€â”€ styles.css          # Styles modernes
â”œâ”€â”€ script.js           # Logique JavaScript
â”œâ”€â”€ server.py           # Serveur Python optionnel
â””â”€â”€ README.md          # Cette documentation
```

## ğŸ”§ Configuration

### Backend API

Par dÃ©faut, l'interface utilise l'endpoint `/completion` compatible llama.cpp.

Pour changer l'URL du backend, modifiez dans `script.js`:

```javascript
// Ligne 9
this.apiUrl = 'http://votre-backend:port/completion';
```

### ParamÃ¨tres de gÃ©nÃ©ration

Les paramÃ¨tres par dÃ©faut (modifiables dans `script.js`):

```javascript
const params = {
    prompt: prompt,
    stream: true,
    n_predict: 800,
    temperature: 0.7,
    top_k: 40,
    top_p: 0.95,
    stop: ["</s>", "Human:", "User:"]
};
```

## ğŸ¨ Personnalisation

### Changer le nom

Remplacez "Joshua" dans `index.html`:

```html
<h1 class="title">VotreNom</h1>
```

### Modifier les couleurs

Dans `styles.css`, les principales variables de couleur:

```css
/* Couleur principale */
.send-btn { background-color: #2563eb; }

/* Messages utilisateur */
.message.user .message-content { background-color: #2563eb; }

/* Style focus */
.input-wrapper:focus-within { border-color: #2563eb; }
```

### Ajouter des thÃ¨mes

Ajoutez des classes CSS pour basculer entre thÃ¨mes clair/sombre.

## ğŸ”Œ IntÃ©gration avec backends

### llama.cpp

Compatible par dÃ©faut. Assurez-vous que llama.cpp server tourne avec:

```bash
# Depuis le dossier llama.cpp
./server -m models/votre-modele.gguf --host 0.0.0.0 --port 8080
```

### Ollama

Modifiez l'endpoint pour Ollama:

```javascript
// Dans script.js
this.apiUrl = 'http://localhost:11434/api/generate';
```

### OpenAI API

Pour l'API OpenAI, adaptez les paramÃ¨tres:

```javascript
const params = {
    messages: [{ role: "user", content: prompt }],
    stream: true,
    model: "gpt-3.5-turbo"
};
```

### API custom

Adaptez le format de requÃªte dans la mÃ©thode `llamaStream()`.

## ğŸ“± FonctionnalitÃ©s avancÃ©es

### Upload d'images

- Cliquez sur ğŸ“ pour uploader
- Support: PNG, JPG, GIF, WebP
- Envoi automatique au backend compatible vision

### Raccourcis clavier

- `Enter` : Envoyer le message
- `Shift + Enter` : Nouvelle ligne
- Auto-redimensionnement du textarea

### Formatage du texte

Support basique Markdown:
- `**gras**` â†’ **gras**
- `*italique*` â†’ *italique*
- `` `code` `` â†’ `code`
- Blocs de code avec ```

## ğŸ› ï¸ DÃ©veloppement

### Modification en temps rÃ©el

1. Modifiez les fichiers CSS/JS
2. Rechargez la page
3. Les changements sont immÃ©diatement visibles

### Debug

Ouvrez la console navigateur (F12) pour:
- Voir les logs de communication API
- DÃ©boguer les erreurs JavaScript
- Monitorer le trafic rÃ©seau

### Tests

Test de l'interface sans backend:
```bash
python3 server.py 8080 http://localhost:9999
```

L'interface affichera des rÃ©ponses de dÃ©monstration.

## ğŸ“Š Performance

- **Taille totale**: ~15KB (non compressÃ©)
- **DÃ©pendances**: Aucune (Vanilla JavaScript)
- **Compatible**: Tous navigateurs modernes
- **Mobile-friendly**: Design responsive

## ğŸ” Troubleshooting

### Erreurs CORS

Si vous voyez des erreurs CORS:
- Utilisez le serveur Python fourni
- Ou servez depuis un serveur HTTP, pas en local `file://`

### Backend non disponible

L'interface montre automatiquement:
- Messages d'erreur clairs
- Mode dÃ©mo avec rÃ©ponses simulÃ©es
- Instructions de connexion

### Performance lente

- RÃ©duisez `n_predict` dans les paramÃ¨tres
- VÃ©rifiez la latence rÃ©seau au backend
- Utilisez un modÃ¨le plus petit

## ğŸ¤ Contribution

Structure modulaire pour faciliter les contributions:

1. **Interface** (`index.html`, `styles.css`)
2. **Logique** (`script.js`)
3. **Backend** (`server.py`)

### Ajout de fonctionnalitÃ©s

- Nouveaux formats de fichiers â†’ `handleFileUpload()`
- ThÃ¨mes â†’ `styles.css`
- APIs â†’ `llamaStream()`

## ğŸ“ Licence

BasÃ© sur le frontend llama.cpp. Code adaptÃ© sous licence MIT.

## ğŸ”— Liens utiles

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Backend AI original
- [Documentation llama.cpp API](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)
- [ModÃ¨les GGUF](https://huggingface.co/models?library=gguf)

---

**Fait avec â¤ï¸ - Interface Joshua v1.0**